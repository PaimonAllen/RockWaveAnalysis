{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "<class '__main__.Model'>\n",
      "\n",
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5245/1356036205.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
      "/tmp/ipykernel_5245/1356036205.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 2163.368042 CroEtr loss: 1.381188\n",
      "training time:  24.12675905227661\n",
      "\n",
      "----------Epoch 2----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 10.303388 CroEtr loss: 1.045081\n",
      "training time:  21.04731297492981\n",
      "\n",
      "----------Epoch 3----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.052157 CroEtr loss: 1.043600\n",
      "training time:  21.10446572303772\n",
      "\n",
      "----------Epoch 4----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.004823 CroEtr loss: 1.043593\n",
      "training time:  21.167033195495605\n",
      "\n",
      "----------Epoch 5----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.001853 CroEtr loss: 1.043592\n",
      "training time:  21.183139085769653\n",
      "\n",
      "----------Epoch 6----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000943 CroEtr loss: 1.043592\n",
      "training time:  21.00264072418213\n",
      "\n",
      "----------Epoch 7----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000862 CroEtr loss: 1.043592\n",
      "training time:  21.119685888290405\n",
      "\n",
      "----------Epoch 8----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000826 CroEtr loss: 1.043592\n",
      "training time:  21.10060954093933\n",
      "\n",
      "----------Epoch 9----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000801 CroEtr loss: 1.043592\n",
      "training time:  21.091090440750122\n",
      "\n",
      "----------Epoch 10----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000777 CroEtr loss: 1.043592\n",
      "training time:  21.164858102798462\n",
      "\n",
      "----------Epoch 11----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000746 CroEtr loss: 1.043592\n",
      "training time:  20.982534170150757\n",
      "\n",
      "----------Epoch 12----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000711 CroEtr loss: 1.043592\n",
      "training time:  21.19719648361206\n",
      "\n",
      "----------Epoch 13----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000674 CroEtr loss: 1.043592\n",
      "training time:  21.132176876068115\n",
      "\n",
      "----------Epoch 14----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000635 CroEtr loss: 1.043592\n",
      "training time:  21.16679072380066\n",
      "\n",
      "----------Epoch 15----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000596 CroEtr loss: 1.043592\n",
      "training time:  21.011117458343506\n",
      "\n",
      "----------Epoch 16----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000239 CroEtr loss: 1.043592\n",
      "training time:  21.14296793937683\n",
      "\n",
      "----------Epoch 17----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000219 CroEtr loss: 1.043592\n",
      "training time:  21.16707706451416\n",
      "\n",
      "----------Epoch 18----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000207 CroEtr loss: 1.043592\n",
      "training time:  21.141496658325195\n",
      "\n",
      "----------Epoch 19----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000193 CroEtr loss: 1.043592\n",
      "training time:  21.194040775299072\n",
      "\n",
      "----------Epoch 20----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000179 CroEtr loss: 1.043592\n",
      "training time:  21.09655237197876\n",
      "\n",
      "----------Epoch 21----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000164 CroEtr loss: 1.043592\n",
      "training time:  21.080556869506836\n",
      "\n",
      "----------Epoch 22----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000149 CroEtr loss: 1.043592\n",
      "training time:  21.059189558029175\n",
      "\n",
      "----------Epoch 23----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000134 CroEtr loss: 1.043592\n",
      "training time:  21.09975028038025\n",
      "\n",
      "----------Epoch 24----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000119 CroEtr loss: 1.043592\n",
      "training time:  21.13584566116333\n",
      "\n",
      "----------Epoch 25----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000105 CroEtr loss: 1.043592\n",
      "training time:  21.02635955810547\n",
      "\n",
      "----------Epoch 26----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000093 CroEtr loss: 1.043592\n",
      "training time:  21.128806829452515\n",
      "\n",
      "----------Epoch 27----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000082 CroEtr loss: 1.043592\n",
      "training time:  21.08428978919983\n",
      "\n",
      "----------Epoch 28----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000072 CroEtr loss: 1.043592\n",
      "training time:  21.0072922706604\n",
      "\n",
      "----------Epoch 29----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000064 CroEtr loss: 1.043592\n",
      "training time:  21.075743675231934\n",
      "\n",
      "----------Epoch 30----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000057 CroEtr loss: 1.043592\n",
      "training time:  21.110347509384155\n",
      "\n",
      "----------Epoch 31----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000051 CroEtr loss: 1.043592\n",
      "training time:  21.06130361557007\n",
      "\n",
      "----------Epoch 32----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000045 CroEtr loss: 1.043592\n",
      "training time:  21.227155208587646\n",
      "\n",
      "----------Epoch 33----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000040 CroEtr loss: 1.043592\n",
      "training time:  21.11176061630249\n",
      "\n",
      "----------Epoch 34----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000036 CroEtr loss: 1.043592\n",
      "training time:  21.1624858379364\n",
      "\n",
      "----------Epoch 35----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000033 CroEtr loss: 1.043592\n",
      "training time:  21.085821390151978\n",
      "\n",
      "----------Epoch 36----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000029 CroEtr loss: 1.043592\n",
      "training time:  21.17967700958252\n",
      "\n",
      "----------Epoch 37----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000026 CroEtr loss: 1.043592\n",
      "training time:  21.131367206573486\n",
      "\n",
      "----------Epoch 38----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000024 CroEtr loss: 1.043592\n",
      "training time:  21.067782402038574\n",
      "\n",
      "----------Epoch 39----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000022 CroEtr loss: 1.043592\n",
      "training time:  21.09122943878174\n",
      "\n",
      "----------Epoch 40----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000020 CroEtr loss: 1.043592\n",
      "training time:  21.175576210021973\n",
      "\n",
      "----------Epoch 41----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000018 CroEtr loss: 1.043592\n",
      "training time:  21.14808225631714\n",
      "\n",
      "----------Epoch 42----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000016 CroEtr loss: 1.043592\n",
      "training time:  21.144524574279785\n",
      "\n",
      "----------Epoch 43----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000014 CroEtr loss: 1.043592\n",
      "training time:  21.061407566070557\n",
      "\n",
      "----------Epoch 44----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000013 CroEtr loss: 1.043592\n",
      "training time:  21.171645641326904\n",
      "\n",
      "----------Epoch 45----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000012 CroEtr loss: 1.043592\n",
      "training time:  21.134848594665527\n",
      "\n",
      "----------Epoch 46----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000011 CroEtr loss: 1.043592\n",
      "training time:  21.17325782775879\n",
      "\n",
      "----------Epoch 47----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000010 CroEtr loss: 1.043592\n",
      "training time:  21.055854558944702\n",
      "\n",
      "----------Epoch 48----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000009 CroEtr loss: 1.043592\n",
      "training time:  21.046934127807617\n",
      "\n",
      "----------Epoch 49----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000008 CroEtr loss: 1.043592\n",
      "training time:  21.102572679519653\n",
      "\n",
      "----------Epoch 50----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000008 CroEtr loss: 1.043592\n",
      "training time:  21.21209406852722\n",
      "\n",
      "----------Epoch 51----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000007 CroEtr loss: 1.043592\n",
      "training time:  21.145496129989624\n",
      "\n",
      "----------Epoch 52----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000007 CroEtr loss: 1.043592\n",
      "training time:  21.099207878112793\n",
      "\n",
      "----------Epoch 53----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000006 CroEtr loss: 1.043592\n",
      "training time:  21.13010263442993\n",
      "\n",
      "----------Epoch 54----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000006 CroEtr loss: 1.043592\n",
      "training time:  21.197373151779175\n",
      "\n",
      "----------Epoch 55----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000005 CroEtr loss: 1.043592\n",
      "training time:  21.148131370544434\n",
      "\n",
      "----------Epoch 56----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000005 CroEtr loss: 1.043592\n",
      "training time:  21.128929615020752\n",
      "\n",
      "----------Epoch 57----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000005 CroEtr loss: 1.043592\n",
      "training time:  21.245260000228882\n",
      "\n",
      "----------Epoch 58----------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "wave=512\n",
    "wavetoChn=int(wave/8)\n",
    "print(wavetoChn)\n",
    "\n",
    "# https://blog.csdn.net/WildCatFish/article/details/116228950\n",
    "\n",
    "def set_random_seed(state=1):\n",
    "    \"\"\"\n",
    "        设定随机种子\n",
    "    :param state: 随机种子值\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    gens = (np.random.seed, torch.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "        \n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "        处理加载的训练数据DataFrame，去掉id，同时对signals以”，“进行拆分。\n",
    "    :param data: DataFrame, shape(n, 3)\n",
    "    :return: np array, shape(n, 206)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        label = data.iloc[i, 2]\n",
    "        x_res.append(label)\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "\n",
    "def get_pred_x(data):\n",
    "    \"\"\"\n",
    "        处理需要预测数据的DataFrame\n",
    "    :param data: DataFrame, shape(n, 2)\n",
    "    :return: np array, shape(n, 205)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "        模型训练部分\n",
    "    :param dataloader: 训练数据集\n",
    "    :param model: 训练用到的模型\n",
    "    :param loss_fn: 评估用的损失函数\n",
    "    :param optimizer: 优化器\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for batch, x_y in enumerate(dataloader):\n",
    "        X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
    "        # 开启梯度\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X.float())\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "        模型测试部分\n",
    "    :param dataloader: 测试数据集\n",
    "    :param model: 测试模型\n",
    "    :param loss_fn: 损失函数\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct, l1_loss = 0, 0, 0\n",
    "    # 用来计算abs-sum. 等于PyTorch L1Loss-->\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss\n",
    "    l1loss_fn = AbsSumLoss()\n",
    "    with torch.no_grad():   # 关掉梯度\n",
    "        model.eval()\n",
    "        for x_y in dataloader:\n",
    "            X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
    "            # 注意Y和y的区别, Y用来计算L1 loss, y是CrossEntropy loss.\n",
    "            Y = torch.zeros(size=(len(y), 6), device='cuda:1')\n",
    "            for i in range(len(Y)):\n",
    "                Y[i][y[i]] = 1\n",
    "\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()    # 这个是CrossEntropy loss\n",
    "            l1_loss += l1loss_fn(pred, Y).item()    # 这个是abs-sum/L1 loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # 这个是计算准确率的, 取概率最大值的下标.\n",
    "\n",
    "    test_loss /= size   # 等于CrossEntropy的reduction='mean', 这里有些多此一举可删掉.\n",
    "    correct /= size\n",
    "    print(f\"Test Results:\\nAccuracy: {(100*correct):>0.1f}% abs-sum loss: {l1_loss:>8f} CroEtr loss: {test_loss:>8f}\")\n",
    "\n",
    "\n",
    "def prediction(net, loss):\n",
    "    \"\"\"\n",
    "        对数据进行预测\n",
    "    :param net: 训练好的模型\n",
    "    :param loss: 模型的测试误差值, 不是损失函数. 可以去掉, 这里是用来给预测数据命名方便区分.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        pred_loader = torch.utils.data.DataLoader(dataset=pred_data)\n",
    "        res = []\n",
    "        for x in pred_loader:\n",
    "            x = torch.tensor(x, device='cuda:1', dtype=torch.float64)\n",
    "            output = net(x.float())\n",
    "            res.append(output.cpu().numpy().tolist())\n",
    "\n",
    "        res = [i[0] for i in res]\n",
    "        res_df = pd.DataFrame(res, columns=[ 'label_1', 'label_2', 'label_3','label_4','label_5','label_6'])\n",
    "        res_df.insert(0, 'id', value=range(100, 120))\n",
    "\n",
    "        res_df.to_csv('res-loss '+str(loss)+'.csv', index=False)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            CNN模型构造\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            # input shape(32, 1, 256) -> [batch_size, channel, features] 8192\n",
    "            # 参考->https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),   # 卷积后(32, 16, 256) 8192\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 下采样down-sampling\n",
    "        self.sampling_layer1 = nn.Sequential(\n",
    "            # input shape(32, 16, 256) 8192\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # size随便选的, 这里output应该是(32, 32, 128) 4096\n",
    "        )\n",
    "\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),   # 输出(32, 64, 128) 4096\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),  # 输出(32, 128, 128)\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 64, 64)\n",
    "        )\n",
    "\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),  # 输出(32, 256, 64)\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),  # 输出(32, 512, 64)\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 512, 32)\n",
    "        )\n",
    "        # 全连接层\n",
    "        self.full_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=512*wavetoChn, out_features=256*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256*wavetoChn, out_features=128*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128*wavetoChn, out_features=64*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64*wavetoChn, out_features=6)\n",
    "        )\n",
    "        # 这个是输出label预测概率, 不知道这写法对不对\n",
    "        self.pred_layer = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "        :param x: batch\n",
    "        :return: training == Ture 返回的是全连接层输出， training == False 加上一个Softmax(), 返回各个label概率.\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(dim=1)  # 升维. input shape(32, 205), output shape(32, 1, 205)\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.sampling_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.sampling_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.sampling_layer3(x)\n",
    "        x = x.view(x.size(0), -1)   # output(32, 12800)\n",
    "        x = self.full_layer(x)\n",
    "\n",
    "        if self.training:\n",
    "            return x    # CrossEntropyLoss自带LogSoftmax, 训练的时候不用输出概率(我也不知道这个写法对不对, 我是试错出来的.)\n",
    "        else:\n",
    "            return self.pred_layer(x)\n",
    "\n",
    "\n",
    "class AbsSumLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            可以直接用PyTorch的nn.L1Loss, 这个我写的时候不知道。\n",
    "        \"\"\"\n",
    "        super(AbsSumLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = F.l1_loss(target, output, reduction='sum')\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_random_seed(1996)   # 设定随机种子\n",
    "    # 加载数据集\n",
    "    data = pd.read_csv('/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/learn/Pytorch-signal/dataset/train3/train3_2.csv')\n",
    "    data = process_data(data)\n",
    "    pred_data = pd.read_csv('/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/learn/Pytorch-signal/dataset/train3/test3.csv')\n",
    "    pred_data = get_pred_x(pred_data)\n",
    "\n",
    "    # 初始化模型\n",
    "    lr_rate = 1e-5\n",
    "    w_decay = 1e-6\n",
    "    n_epoch = 100\n",
    "    b_size = 1024\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    net = Model()\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=lr_rate, weight_decay=w_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    print(Model)\n",
    "\n",
    "    # 拆分训练测试集\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    train, test = torch.cuda.FloatTensor(train), torch.cuda.FloatTensor(test)\n",
    "    train=train.to(device)\n",
    "    test=test.to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=b_size)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=b_size)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start = time.time()\n",
    "        print(f\"\\n----------Epoch {epoch + 1}----------\")\n",
    "        train_loop(train_loader, net, loss_fn, optimizer)\n",
    "        test_loop(test_loader, net, loss_fn)\n",
    "        end = time.time()\n",
    "        print('training time: ', end-start)\n",
    "\n",
    "    # predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "<class '__main__.Model'>\n",
      "\n",
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5245/1356036205.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
      "/tmp/ipykernel_5245/1356036205.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 2163.368042 CroEtr loss: 1.381188\n",
      "training time:  24.12675905227661\n",
      "\n",
      "----------Epoch 2----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 10.303388 CroEtr loss: 1.045081\n",
      "training time:  21.04731297492981\n",
      "\n",
      "----------Epoch 3----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.052157 CroEtr loss: 1.043600\n",
      "training time:  21.10446572303772\n",
      "\n",
      "----------Epoch 4----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.004823 CroEtr loss: 1.043593\n",
      "training time:  21.167033195495605\n",
      "\n",
      "----------Epoch 5----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.001853 CroEtr loss: 1.043592\n",
      "training time:  21.183139085769653\n",
      "\n",
      "----------Epoch 6----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000943 CroEtr loss: 1.043592\n",
      "training time:  21.00264072418213\n",
      "\n",
      "----------Epoch 7----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000862 CroEtr loss: 1.043592\n",
      "training time:  21.119685888290405\n",
      "\n",
      "----------Epoch 8----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000826 CroEtr loss: 1.043592\n",
      "training time:  21.10060954093933\n",
      "\n",
      "----------Epoch 9----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000801 CroEtr loss: 1.043592\n",
      "training time:  21.091090440750122\n",
      "\n",
      "----------Epoch 10----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000777 CroEtr loss: 1.043592\n",
      "training time:  21.164858102798462\n",
      "\n",
      "----------Epoch 11----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000746 CroEtr loss: 1.043592\n",
      "training time:  20.982534170150757\n",
      "\n",
      "----------Epoch 12----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000711 CroEtr loss: 1.043592\n",
      "training time:  21.19719648361206\n",
      "\n",
      "----------Epoch 13----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000674 CroEtr loss: 1.043592\n",
      "training time:  21.132176876068115\n",
      "\n",
      "----------Epoch 14----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000635 CroEtr loss: 1.043592\n",
      "training time:  21.16679072380066\n",
      "\n",
      "----------Epoch 15----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000596 CroEtr loss: 1.043592\n",
      "training time:  21.011117458343506\n",
      "\n",
      "----------Epoch 16----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000239 CroEtr loss: 1.043592\n",
      "training time:  21.14296793937683\n",
      "\n",
      "----------Epoch 17----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000219 CroEtr loss: 1.043592\n",
      "training time:  21.16707706451416\n",
      "\n",
      "----------Epoch 18----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000207 CroEtr loss: 1.043592\n",
      "training time:  21.141496658325195\n",
      "\n",
      "----------Epoch 19----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000193 CroEtr loss: 1.043592\n",
      "training time:  21.194040775299072\n",
      "\n",
      "----------Epoch 20----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000179 CroEtr loss: 1.043592\n",
      "training time:  21.09655237197876\n",
      "\n",
      "----------Epoch 21----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000164 CroEtr loss: 1.043592\n",
      "training time:  21.080556869506836\n",
      "\n",
      "----------Epoch 22----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000149 CroEtr loss: 1.043592\n",
      "training time:  21.059189558029175\n",
      "\n",
      "----------Epoch 23----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000134 CroEtr loss: 1.043592\n",
      "training time:  21.09975028038025\n",
      "\n",
      "----------Epoch 24----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000119 CroEtr loss: 1.043592\n",
      "training time:  21.13584566116333\n",
      "\n",
      "----------Epoch 25----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000105 CroEtr loss: 1.043592\n",
      "training time:  21.02635955810547\n",
      "\n",
      "----------Epoch 26----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000093 CroEtr loss: 1.043592\n",
      "training time:  21.128806829452515\n",
      "\n",
      "----------Epoch 27----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000082 CroEtr loss: 1.043592\n",
      "training time:  21.08428978919983\n",
      "\n",
      "----------Epoch 28----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000072 CroEtr loss: 1.043592\n",
      "training time:  21.0072922706604\n",
      "\n",
      "----------Epoch 29----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000064 CroEtr loss: 1.043592\n",
      "training time:  21.075743675231934\n",
      "\n",
      "----------Epoch 30----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000057 CroEtr loss: 1.043592\n",
      "training time:  21.110347509384155\n",
      "\n",
      "----------Epoch 31----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000051 CroEtr loss: 1.043592\n",
      "training time:  21.06130361557007\n",
      "\n",
      "----------Epoch 32----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000045 CroEtr loss: 1.043592\n",
      "training time:  21.227155208587646\n",
      "\n",
      "----------Epoch 33----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000040 CroEtr loss: 1.043592\n",
      "training time:  21.11176061630249\n",
      "\n",
      "----------Epoch 34----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000036 CroEtr loss: 1.043592\n",
      "training time:  21.1624858379364\n",
      "\n",
      "----------Epoch 35----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000033 CroEtr loss: 1.043592\n",
      "training time:  21.085821390151978\n",
      "\n",
      "----------Epoch 36----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000029 CroEtr loss: 1.043592\n",
      "training time:  21.17967700958252\n",
      "\n",
      "----------Epoch 37----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000026 CroEtr loss: 1.043592\n",
      "training time:  21.131367206573486\n",
      "\n",
      "----------Epoch 38----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000024 CroEtr loss: 1.043592\n",
      "training time:  21.067782402038574\n",
      "\n",
      "----------Epoch 39----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000022 CroEtr loss: 1.043592\n",
      "training time:  21.09122943878174\n",
      "\n",
      "----------Epoch 40----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000020 CroEtr loss: 1.043592\n",
      "training time:  21.175576210021973\n",
      "\n",
      "----------Epoch 41----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000018 CroEtr loss: 1.043592\n",
      "training time:  21.14808225631714\n",
      "\n",
      "----------Epoch 42----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000016 CroEtr loss: 1.043592\n",
      "training time:  21.144524574279785\n",
      "\n",
      "----------Epoch 43----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000014 CroEtr loss: 1.043592\n",
      "training time:  21.061407566070557\n",
      "\n",
      "----------Epoch 44----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000013 CroEtr loss: 1.043592\n",
      "training time:  21.171645641326904\n",
      "\n",
      "----------Epoch 45----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000012 CroEtr loss: 1.043592\n",
      "training time:  21.134848594665527\n",
      "\n",
      "----------Epoch 46----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000011 CroEtr loss: 1.043592\n",
      "training time:  21.17325782775879\n",
      "\n",
      "----------Epoch 47----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000010 CroEtr loss: 1.043592\n",
      "training time:  21.055854558944702\n",
      "\n",
      "----------Epoch 48----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000009 CroEtr loss: 1.043592\n",
      "training time:  21.046934127807617\n",
      "\n",
      "----------Epoch 49----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000008 CroEtr loss: 1.043592\n",
      "training time:  21.102572679519653\n",
      "\n",
      "----------Epoch 50----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000008 CroEtr loss: 1.043592\n",
      "training time:  21.21209406852722\n",
      "\n",
      "----------Epoch 51----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000007 CroEtr loss: 1.043592\n",
      "training time:  21.145496129989624\n",
      "\n",
      "----------Epoch 52----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000007 CroEtr loss: 1.043592\n",
      "training time:  21.099207878112793\n",
      "\n",
      "----------Epoch 53----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000006 CroEtr loss: 1.043592\n",
      "training time:  21.13010263442993\n",
      "\n",
      "----------Epoch 54----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000006 CroEtr loss: 1.043592\n",
      "training time:  21.197373151779175\n",
      "\n",
      "----------Epoch 55----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000005 CroEtr loss: 1.043592\n",
      "training time:  21.148131370544434\n",
      "\n",
      "----------Epoch 56----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000005 CroEtr loss: 1.043592\n",
      "training time:  21.128929615020752\n",
      "\n",
      "----------Epoch 57----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.000005 CroEtr loss: 1.043592\n",
      "training time:  21.245260000228882\n",
      "\n",
      "----------Epoch 58----------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "wave=512\n",
    "wavetoChn=int(wave/8)\n",
    "print(wavetoChn)\n",
    "\n",
    "# https://blog.csdn.net/WildCatFish/article/details/116228950\n",
    "\n",
    "def set_random_seed(state=1):\n",
    "    \"\"\"\n",
    "        设定随机种子\n",
    "    :param state: 随机种子值\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    gens = (np.random.seed, torch.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "        \n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "        处理加载的训练数据DataFrame，去掉id，同时对signals以”，“进行拆分。\n",
    "    :param data: DataFrame, shape(n, 3)\n",
    "    :return: np array, shape(n, 206)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        label = data.iloc[i, 2]\n",
    "        x_res.append(label)\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "\n",
    "def get_pred_x(data):\n",
    "    \"\"\"\n",
    "        处理需要预测数据的DataFrame\n",
    "    :param data: DataFrame, shape(n, 2)\n",
    "    :return: np array, shape(n, 205)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "        模型训练部分\n",
    "    :param dataloader: 训练数据集\n",
    "    :param model: 训练用到的模型\n",
    "    :param loss_fn: 评估用的损失函数\n",
    "    :param optimizer: 优化器\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for batch, x_y in enumerate(dataloader):\n",
    "        X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
    "        # 开启梯度\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X.float())\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "        模型测试部分\n",
    "    :param dataloader: 测试数据集\n",
    "    :param model: 测试模型\n",
    "    :param loss_fn: 损失函数\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct, l1_loss = 0, 0, 0\n",
    "    # 用来计算abs-sum. 等于PyTorch L1Loss-->\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss\n",
    "    l1loss_fn = AbsSumLoss()\n",
    "    with torch.no_grad():   # 关掉梯度\n",
    "        model.eval()\n",
    "        for x_y in dataloader:\n",
    "            X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
    "            # 注意Y和y的区别, Y用来计算L1 loss, y是CrossEntropy loss.\n",
    "            Y = torch.zeros(size=(len(y), 6), device='cuda:1')\n",
    "            for i in range(len(Y)):\n",
    "                Y[i][y[i]] = 1\n",
    "\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()    # 这个是CrossEntropy loss\n",
    "            l1_loss += l1loss_fn(pred, Y).item()    # 这个是abs-sum/L1 loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # 这个是计算准确率的, 取概率最大值的下标.\n",
    "\n",
    "    test_loss /= size   # 等于CrossEntropy的reduction='mean', 这里有些多此一举可删掉.\n",
    "    correct /= size\n",
    "    print(f\"Test Results:\\nAccuracy: {(100*correct):>0.1f}% abs-sum loss: {l1_loss:>8f} CroEtr loss: {test_loss:>8f}\")\n",
    "\n",
    "\n",
    "def prediction(net, loss):\n",
    "    \"\"\"\n",
    "        对数据进行预测\n",
    "    :param net: 训练好的模型\n",
    "    :param loss: 模型的测试误差值, 不是损失函数. 可以去掉, 这里是用来给预测数据命名方便区分.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        pred_loader = torch.utils.data.DataLoader(dataset=pred_data)\n",
    "        res = []\n",
    "        for x in pred_loader:\n",
    "            x = torch.tensor(x, device='cuda:1', dtype=torch.float64)\n",
    "            output = net(x.float())\n",
    "            res.append(output.cpu().numpy().tolist())\n",
    "\n",
    "        res = [i[0] for i in res]\n",
    "        res_df = pd.DataFrame(res, columns=[ 'label_1', 'label_2', 'label_3','label_4','label_5','label_6'])\n",
    "        res_df.insert(0, 'id', value=range(100, 120))\n",
    "\n",
    "        res_df.to_csv('res-loss '+str(loss)+'.csv', index=False)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            CNN模型构造\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            # input shape(32, 1, 256) -> [batch_size, channel, features] 8192\n",
    "            # 参考->https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),   # 卷积后(32, 16, 256) 8192\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 下采样down-sampling\n",
    "        self.sampling_layer1 = nn.Sequential(\n",
    "            # input shape(32, 16, 256) 8192\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # size随便选的, 这里output应该是(32, 32, 128) 4096\n",
    "        )\n",
    "\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),   # 输出(32, 64, 128) 4096\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),  # 输出(32, 128, 128)\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 64, 64)\n",
    "        )\n",
    "\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),  # 输出(32, 256, 64)\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),  # 输出(32, 512, 64)\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 512, 32)\n",
    "        )\n",
    "        # 全连接层\n",
    "        self.full_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=512*wavetoChn, out_features=256*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256*wavetoChn, out_features=128*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128*wavetoChn, out_features=64*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64*wavetoChn, out_features=6)\n",
    "        )\n",
    "        # 这个是输出label预测概率, 不知道这写法对不对\n",
    "        self.pred_layer = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "        :param x: batch\n",
    "        :return: training == Ture 返回的是全连接层输出， training == False 加上一个Softmax(), 返回各个label概率.\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(dim=1)  # 升维. input shape(32, 205), output shape(32, 1, 205)\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.sampling_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.sampling_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.sampling_layer3(x)\n",
    "        x = x.view(x.size(0), -1)   # output(32, 12800)\n",
    "        x = self.full_layer(x)\n",
    "\n",
    "        if self.training:\n",
    "            return x    # CrossEntropyLoss自带LogSoftmax, 训练的时候不用输出概率(我也不知道这个写法对不对, 我是试错出来的.)\n",
    "        else:\n",
    "            return self.pred_layer(x)\n",
    "\n",
    "\n",
    "class AbsSumLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            可以直接用PyTorch的nn.L1Loss, 这个我写的时候不知道。\n",
    "        \"\"\"\n",
    "        super(AbsSumLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = F.l1_loss(target, output, reduction='sum')\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_random_seed(1996)   # 设定随机种子\n",
    "    # 加载数据集\n",
    "    data = pd.read_csv('/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/learn/Pytorch-signal/dataset/train3/train3_2.csv')\n",
    "    data = process_data(data)\n",
    "    pred_data = pd.read_csv('/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/learn/Pytorch-signal/dataset/train3/test3.csv')\n",
    "    pred_data = get_pred_x(pred_data)\n",
    "\n",
    "    # 初始化模型\n",
    "    lr_rate = 1e-5\n",
    "    w_decay = 1e-6\n",
    "    n_epoch = 100\n",
    "    b_size = 1024\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    net = Model()\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=lr_rate, weight_decay=w_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    print(Model)\n",
    "\n",
    "    # 拆分训练测试集\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    train, test = torch.cuda.FloatTensor(train), torch.cuda.FloatTensor(test)\n",
    "    train=train.to(device)\n",
    "    test=test.to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=b_size)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=b_size)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start = time.time()\n",
    "        print(f\"\\n----------Epoch {epoch + 1}----------\")\n",
    "        train_loop(train_loader, net, loss_fn, optimizer)\n",
    "        test_loop(test_loader, net, loss_fn)\n",
    "        end = time.time()\n",
    "        print('training time: ', end-start)\n",
    "\n",
    "    # predict\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('RockLabTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "527e757e1c587730d72dfd6dc7ea9b8408877a8a8e3b307a272307089f882730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
