{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "\n",
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_167559/1624965918.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
      "/home/tzr/anaconda3/envs/RockLabTorch/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/tmp/ipykernel_167559/1624965918.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.0% abs-sum loss: 332.921875 CroEtr loss: 1.790732\n",
      "training time:  1.375952959060669\n",
      "\n",
      "----------Epoch 2----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 331.992950 CroEtr loss: 1.788410\n",
      "training time:  1.2938835620880127\n",
      "\n",
      "----------Epoch 3----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 331.085938 CroEtr loss: 1.786145\n",
      "training time:  1.2905049324035645\n",
      "\n",
      "----------Epoch 4----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 330.157288 CroEtr loss: 1.783826\n",
      "training time:  1.2919907569885254\n",
      "\n",
      "----------Epoch 5----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 329.195312 CroEtr loss: 1.781426\n",
      "training time:  1.2921538352966309\n",
      "\n",
      "----------Epoch 6----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 328.178162 CroEtr loss: 1.778889\n",
      "training time:  1.2899267673492432\n",
      "\n",
      "----------Epoch 7----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 327.083008 CroEtr loss: 1.776159\n",
      "training time:  1.2903597354888916\n",
      "\n",
      "----------Epoch 8----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 325.889526 CroEtr loss: 1.773186\n",
      "training time:  1.2885887622833252\n",
      "\n",
      "----------Epoch 9----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 324.577698 CroEtr loss: 1.769919\n",
      "training time:  1.2922990322113037\n",
      "\n",
      "----------Epoch 10----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 323.107483 CroEtr loss: 1.766261\n",
      "training time:  1.2908201217651367\n",
      "\n",
      "----------Epoch 11----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 321.449066 CroEtr loss: 1.762139\n",
      "training time:  1.2929966449737549\n",
      "\n",
      "----------Epoch 12----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 319.570557 CroEtr loss: 1.757473\n",
      "training time:  1.2885253429412842\n",
      "\n",
      "----------Epoch 13----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 317.425659 CroEtr loss: 1.752151\n",
      "training time:  1.2902085781097412\n",
      "\n",
      "----------Epoch 14----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 314.966736 CroEtr loss: 1.746057\n",
      "training time:  1.2929065227508545\n",
      "\n",
      "----------Epoch 15----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 312.135071 CroEtr loss: 1.739050\n",
      "training time:  1.293686866760254\n",
      "\n",
      "----------Epoch 16----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 308.854309 CroEtr loss: 1.730943\n",
      "training time:  1.2926485538482666\n",
      "\n",
      "----------Epoch 17----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 305.036194 CroEtr loss: 1.721528\n",
      "training time:  1.2904701232910156\n",
      "\n",
      "----------Epoch 18----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 300.567139 CroEtr loss: 1.710531\n",
      "training time:  1.287782907485962\n",
      "\n",
      "----------Epoch 19----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 295.312988 CroEtr loss: 1.697637\n",
      "training time:  1.2902143001556396\n",
      "\n",
      "----------Epoch 20----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 289.118988 CroEtr loss: 1.682483\n",
      "training time:  1.2918472290039062\n",
      "\n",
      "----------Epoch 21----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 281.806030 CroEtr loss: 1.664660\n",
      "training time:  1.2928998470306396\n",
      "\n",
      "----------Epoch 22----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 273.154449 CroEtr loss: 1.643669\n",
      "training time:  1.2905938625335693\n",
      "\n",
      "----------Epoch 23----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 262.913666 CroEtr loss: 1.618959\n",
      "training time:  1.2869768142700195\n",
      "\n",
      "----------Epoch 24----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 250.801025 CroEtr loss: 1.589925\n",
      "training time:  1.2904078960418701\n",
      "\n",
      "----------Epoch 25----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 236.528534 CroEtr loss: 1.555990\n",
      "training time:  1.2923195362091064\n",
      "\n",
      "----------Epoch 26----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 219.833649 CroEtr loss: 1.516683\n",
      "training time:  1.2872507572174072\n",
      "\n",
      "----------Epoch 27----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 200.553101 CroEtr loss: 1.471824\n",
      "training time:  1.2916302680969238\n",
      "\n",
      "----------Epoch 28----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 178.719482 CroEtr loss: 1.421740\n",
      "training time:  1.291914701461792\n",
      "\n",
      "----------Epoch 29----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 154.667603 CroEtr loss: 1.367477\n",
      "training time:  1.2877585887908936\n",
      "\n",
      "----------Epoch 30----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 129.180023 CroEtr loss: 1.311055\n",
      "training time:  1.2883951663970947\n",
      "\n",
      "----------Epoch 31----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 103.477798 CroEtr loss: 1.255323\n",
      "training time:  1.2911741733551025\n",
      "\n",
      "----------Epoch 32----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 79.108688 CroEtr loss: 1.203600\n",
      "training time:  1.291954755783081\n",
      "\n",
      "----------Epoch 33----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 57.601509 CroEtr loss: 1.158883\n",
      "training time:  1.2905306816101074\n",
      "\n",
      "----------Epoch 34----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 40.020126 CroEtr loss: 1.122993\n",
      "training time:  1.290947675704956\n",
      "\n",
      "----------Epoch 35----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 26.689964 CroEtr loss: 1.096188\n",
      "training time:  1.289844274520874\n",
      "\n",
      "----------Epoch 36----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 17.244526 CroEtr loss: 1.077409\n",
      "training time:  1.2907683849334717\n",
      "\n",
      "----------Epoch 37----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 10.910499 CroEtr loss: 1.064917\n",
      "training time:  1.291802167892456\n",
      "\n",
      "----------Epoch 38----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 6.829515 CroEtr loss: 1.056912\n",
      "training time:  1.2918505668640137\n",
      "\n",
      "----------Epoch 39----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 4.268096 CroEtr loss: 1.051905\n",
      "training time:  1.2925488948822021\n",
      "\n",
      "----------Epoch 40----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 2.682304 CroEtr loss: 1.048812\n",
      "training time:  1.2863948345184326\n",
      "\n",
      "----------Epoch 41----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 1.704708 CroEtr loss: 1.046908\n",
      "training time:  1.2889063358306885\n",
      "\n",
      "----------Epoch 42----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 1.100053 CroEtr loss: 1.045731\n",
      "training time:  1.2904889583587646\n",
      "\n",
      "----------Epoch 43----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.722785 CroEtr loss: 1.044997\n",
      "training time:  1.290637493133545\n",
      "\n",
      "----------Epoch 44----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.484503 CroEtr loss: 1.044534\n",
      "training time:  1.2919762134552002\n",
      "\n",
      "----------Epoch 45----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.331723 CroEtr loss: 1.044237\n",
      "training time:  1.2908320426940918\n",
      "\n",
      "----------Epoch 46----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.232094 CroEtr loss: 1.044043\n",
      "training time:  1.2901155948638916\n",
      "\n",
      "----------Epoch 47----------\n",
      "Test Results:\n",
      "Accuracy: 100.0% abs-sum loss: 0.165972 CroEtr loss: 1.043914\n",
      "training time:  1.2899346351623535\n",
      "\n",
      "----------Epoch 48----------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "wave=512\n",
    "wavetoChn=int(wave/8)\n",
    "print(wavetoChn)\n",
    "\n",
    "# https://blog.csdn.net/WildCatFish/article/details/116228950\n",
    "\n",
    "def set_random_seed(state=1):\n",
    "    \"\"\"\n",
    "        设定随机种子\n",
    "    :param state: 随机种子值\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    gens = (np.random.seed, torch.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "        \n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "        处理加载的训练数据DataFrame，去掉id，同时对signals以”，“进行拆分。\n",
    "    :param data: DataFrame, shape(n, 3)\n",
    "    :return: np array, shape(n, 206)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        label = data.iloc[i, 2]\n",
    "        x_res.append(label)\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "\n",
    "def get_pred_x(data):\n",
    "    \"\"\"\n",
    "        处理需要预测数据的DataFrame\n",
    "    :param data: DataFrame, shape(n, 2)\n",
    "    :return: np array, shape(n, 205)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "        模型训练部分\n",
    "    :param dataloader: 训练数据集\n",
    "    :param model: 训练用到的模型\n",
    "    :param loss_fn: 评估用的损失函数\n",
    "    :param optimizer: 优化器\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for batch, x_y in enumerate(dataloader):\n",
    "        X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
    "        # 开启梯度\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X.float())\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "        模型测试部分\n",
    "    :param dataloader: 测试数据集\n",
    "    :param model: 测试模型\n",
    "    :param loss_fn: 损失函数\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct, l1_loss = 0, 0, 0\n",
    "    # 用来计算abs-sum. 等于PyTorch L1Loss-->\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss\n",
    "    l1loss_fn = AbsSumLoss()\n",
    "    with torch.no_grad():   # 关掉梯度\n",
    "        model.eval()\n",
    "        for x_y in dataloader:\n",
    "            X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
    "            # 注意Y和y的区别, Y用来计算L1 loss, y是CrossEntropy loss.\n",
    "            Y = torch.zeros(size=(len(y), 6), device='cuda:1')\n",
    "            for i in range(len(Y)):\n",
    "                Y[i][y[i]] = 1\n",
    "\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()    # 这个是CrossEntropy loss\n",
    "            l1_loss += l1loss_fn(pred, Y).item()    # 这个是abs-sum/L1 loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # 这个是计算准确率的, 取概率最大值的下标.\n",
    "\n",
    "    test_loss /= size   # 等于CrossEntropy的reduction='mean', 这里有些多此一举可删掉.\n",
    "    correct /= size\n",
    "    print(f\"Test Results:\\nAccuracy: {(100*correct):>0.1f}% abs-sum loss: {l1_loss:>8f} CroEtr loss: {test_loss:>8f}\")\n",
    "\n",
    "\n",
    "def prediction(net, loss):\n",
    "    \"\"\"\n",
    "        对数据进行预测\n",
    "    :param net: 训练好的模型\n",
    "    :param loss: 模型的测试误差值, 不是损失函数. 可以去掉, 这里是用来给预测数据命名方便区分.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        pred_loader = torch.utils.data.DataLoader(dataset=pred_data)\n",
    "        res = []\n",
    "        for x in pred_loader:\n",
    "            x = torch.tensor(x, device='cuda:1', dtype=torch.float64)\n",
    "            output = net(x.float())\n",
    "            res.append(output.cpu().numpy().tolist())\n",
    "\n",
    "        res = [i[0] for i in res]\n",
    "        res_df = pd.DataFrame(res, columns=[ 'label_1', 'label_2', 'label_3','label_4','label_5','label_6'])\n",
    "        res_df.insert(0, 'id', value=range(100, 120))\n",
    "\n",
    "        res_df.to_csv('res-loss '+str(loss)+'.csv', index=False)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            CNN模型构造\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            # input shape(32, 1, 256) -> [batch_size, channel, features] 8192\n",
    "            # 参考->https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),   # 卷积后(32, 16, 256) 8192\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 下采样down-sampling\n",
    "        self.sampling_layer1 = nn.Sequential(\n",
    "            # input shape(32, 16, 256) 8192\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # size随便选的, 这里output应该是(32, 32, 128) 4096\n",
    "        )\n",
    "\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),   # 输出(32, 64, 128) 4096\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),  # 输出(32, 128, 128)\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 64, 64)\n",
    "        )\n",
    "\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),  # 输出(32, 256, 64)\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),  # 输出(32, 512, 64)\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 512, 32)\n",
    "        )\n",
    "        # 全连接层\n",
    "        self.full_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=512*wavetoChn, out_features=256*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256*wavetoChn, out_features=128*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128*wavetoChn, out_features=64*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64*wavetoChn, out_features=6)\n",
    "        )\n",
    "        # 这个是输出label预测概率, 不知道这写法对不对\n",
    "        self.pred_layer = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "        :param x: batch\n",
    "        :return: training == Ture 返回的是全连接层输出， training == False 加上一个Softmax(), 返回各个label概率.\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(dim=1)  # 升维. input shape(32, 205), output shape(32, 1, 205)\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.sampling_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.sampling_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.sampling_layer3(x)\n",
    "        x = x.view(x.size(0), -1)   # output(32, 12800)\n",
    "        x = self.full_layer(x)\n",
    "\n",
    "        if self.training:\n",
    "            return x    # CrossEntropyLoss自带LogSoftmax, 训练的时候不用输出概率(我也不知道这个写法对不对, 我是试错出来的.)\n",
    "        else:\n",
    "            return self.pred_layer(x)\n",
    "\n",
    "\n",
    "class AbsSumLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            可以直接用PyTorch的nn.L1Loss, 这个我写的时候不知道。\n",
    "        \"\"\"\n",
    "        super(AbsSumLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = F.l1_loss(target, output, reduction='sum')\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_random_seed(1996)   # 设定随机种子\n",
    "    # 加载数据集\n",
    "    data = pd.read_csv('./dataset/train1/train1.csv')\n",
    "    data = process_data(data)\n",
    "    pred_data = pd.read_csv('./dataset/train1/test1.csv')\n",
    "    pred_data = get_pred_x(pred_data)\n",
    "\n",
    "    # 初始化模型\n",
    "    lr_rate = 1e-5\n",
    "    w_decay = 1e-6\n",
    "    n_epoch = 100\n",
    "    b_size = 1024\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    net = Model()\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=lr_rate, weight_decay=w_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    print(Model)\n",
    "\n",
    "    # 拆分训练测试集\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "    train, test = torch.cuda.FloatTensor(train), torch.cuda.FloatTensor(test)\n",
    "    train=train.to(device)\n",
    "    test=test.to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=b_size)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=b_size)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start = time.time()\n",
    "        print(f\"\\n----------Epoch {epoch + 1}----------\")\n",
    "        train_loop(train_loader, net, loss_fn, optimizer)\n",
    "        test_loop(test_loader, net, loss_fn)\n",
    "        end = time.time()\n",
    "        print('training time: ', end-start)\n",
    "\n",
    "    # predict\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('RockLabTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "527e757e1c587730d72dfd6dc7ea9b8408877a8a8e3b307a272307089f882730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
