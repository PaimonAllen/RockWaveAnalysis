{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "<class '__main__.Model'>\n",
      "\n",
      "----------Epoch 1----------\n",
      "torch.Size([1, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1107668/642192204.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 16, 256]             --\n",
      "|    └─Conv1d: 2-1                       [-1, 16, 256]             64\n",
      "|    └─BatchNorm1d: 2-2                  [-1, 16, 256]             32\n",
      "|    └─ReLU: 2-3                         [-1, 16, 256]             --\n",
      "├─Sequential: 1-2                        [-1, 32, 128]             --\n",
      "|    └─Conv1d: 2-4                       [-1, 32, 256]             1,568\n",
      "|    └─BatchNorm1d: 2-5                  [-1, 32, 256]             64\n",
      "|    └─ReLU: 2-6                         [-1, 32, 256]             --\n",
      "|    └─MaxPool1d: 2-7                    [-1, 32, 128]             --\n",
      "├─Sequential: 1-3                        [-1, 64, 128]             --\n",
      "|    └─Conv1d: 2-8                       [-1, 64, 128]             6,208\n",
      "|    └─BatchNorm1d: 2-9                  [-1, 64, 128]             128\n",
      "|    └─ReLU: 2-10                        [-1, 64, 128]             --\n",
      "├─Sequential: 1-4                        [-1, 128, 64]             --\n",
      "|    └─Conv1d: 2-11                      [-1, 128, 128]            24,704\n",
      "|    └─BatchNorm1d: 2-12                 [-1, 128, 128]            256\n",
      "|    └─ReLU: 2-13                        [-1, 128, 128]            --\n",
      "|    └─MaxPool1d: 2-14                   [-1, 128, 64]             --\n",
      "├─Sequential: 1-5                        [-1, 256, 64]             --\n",
      "|    └─Conv1d: 2-15                      [-1, 256, 64]             98,560\n",
      "|    └─BatchNorm1d: 2-16                 [-1, 256, 64]             512\n",
      "|    └─ReLU: 2-17                        [-1, 256, 64]             --\n",
      "├─Sequential: 1-6                        [-1, 512, 32]             --\n",
      "|    └─Conv1d: 2-18                      [-1, 512, 64]             393,728\n",
      "|    └─BatchNorm1d: 2-19                 [-1, 512, 64]             1,024\n",
      "|    └─ReLU: 2-20                        [-1, 512, 64]             --\n",
      "|    └─MaxPool1d: 2-21                   [-1, 512, 32]             --\n",
      "├─Sequential: 1-7                        [-1, 6]                   --\n",
      "|    └─Linear: 2-22                      [-1, 8192]                134,225,920\n",
      "|    └─ReLU: 2-23                        [-1, 8192]                --\n",
      "|    └─Linear: 2-24                      [-1, 4096]                33,558,528\n",
      "|    └─ReLU: 2-25                        [-1, 4096]                --\n",
      "|    └─Linear: 2-26                      [-1, 2048]                8,390,656\n",
      "|    └─ReLU: 2-27                        [-1, 2048]                --\n",
      "|    └─Linear: 2-28                      [-1, 6]                   12,294\n",
      "├─Softmax: 1-8                           [-1, 6]                   --\n",
      "==========================================================================================\n",
      "Total params: 176,714,246\n",
      "Trainable params: 176,714,246\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 388.67\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.42\n",
      "Params size (MB): 674.11\n",
      "Estimated Total Size (MB): 675.53\n",
      "==========================================================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper__cudnn_batch_norm_backward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 232>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb#ch0000000vscode-remote?line=280'>281</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb#ch0000000vscode-remote?line=281'>282</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m----------Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m----------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb#ch0000000vscode-remote?line=282'>283</a>\u001b[0m train_loop(train_loader, net, loss_fn, optimizer)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb#ch0000000vscode-remote?line=283'>284</a>\u001b[0m test_loop(test_loader, net, loss_fn)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb#ch0000000vscode-remote?line=284'>285</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32m/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb Cell 1\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb#ch0000000vscode-remote?line=74'>75</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb#ch0000000vscode-remote?line=75'>76</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb#ch0000000vscode-remote?line=76'>77</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1D-visualization_256.ipynb#ch0000000vscode-remote?line=77'>78</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper__cudnn_batch_norm_backward)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import hiddenlayer as h\n",
    "import sys\n",
    "from torchviz import make_dot\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "wave=256\n",
    "wavetoChn=int(wave/8)\n",
    "print(wavetoChn)\n",
    "\n",
    "# https://blog.csdn.net/WildCatFish/article/details/116228950\n",
    "\n",
    "def set_random_seed(state=1):\n",
    "    \"\"\"\n",
    "        设定随机种子\n",
    "    :param state: 随机种子值\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    gens = (np.random.seed, torch.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "        \n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "        处理加载的训练数据DataFrame，去掉id，同时对signals以”，“进行拆分。\n",
    "    :param data: DataFrame, shape(n, 3)\n",
    "    :return: np array, shape(n, 206)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        label = data.iloc[i, 2]\n",
    "        x_res.append(label)\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "\n",
    "def get_pred_x(data):\n",
    "    \"\"\"\n",
    "        处理需要预测数据的DataFrame\n",
    "    :param data: DataFrame, shape(n, 2)\n",
    "    :return: np array, shape(n, 205)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "        模型训练部分\n",
    "    :param dataloader: 训练数据集\n",
    "    :param model: 训练用到的模型\n",
    "    :param loss_fn: 评估用的损失函数\n",
    "    :param optimizer: 优化器\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for batch, x_y in enumerate(dataloader):\n",
    "        X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
    "        # 开启梯度\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Compute prediction and loss\n",
    "            print(X.shape)\n",
    "            pred = model(X.float())\n",
    "            summary(model,X.float())\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "        模型测试部分\n",
    "    :param dataloader: 测试数据集\n",
    "    :param model: 测试模型\n",
    "    :param loss_fn: 损失函数\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct, l1_loss = 0, 0, 0\n",
    "    # 用来计算abs-sum. 等于PyTorch L1Loss-->\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss\n",
    "    l1loss_fn = AbsSumLoss()\n",
    "    with torch.no_grad():   # 关掉梯度\n",
    "        model.eval()\n",
    "        for x_y in dataloader:\n",
    "            X, y = x_y[:, :wave].type(torch.float64), torch.tensor(x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
    "            # 注意Y和y的区别, Y用来计算L1 loss, y是CrossEntropy loss.\n",
    "            Y = torch.zeros(size=(len(y), 6), device='cuda:1')\n",
    "            for i in range(len(Y)):\n",
    "                Y[i][y[i]] = 1\n",
    "\n",
    "            pred = model(X.float())\n",
    "            # summary(pred)\n",
    "            test_loss += loss_fn(pred, y).item()    # 这个是CrossEntropy loss\n",
    "            l1_loss += l1loss_fn(pred, Y).item()    # 这个是abs-sum/L1 loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()  # 这个是计算准确率的, 取概率最大值的下标.\n",
    "\n",
    "    test_loss /= size   # 等于CrossEntropy的reduction='mean', 这里有些多此一举可删掉.\n",
    "    correct /= size\n",
    "    print(f\"Test Results:\\nAccuracy: {(100*correct):>0.1f}% abs-sum loss: {l1_loss:>8f} CroEtr loss: {test_loss:>8f}\")\n",
    "\n",
    "\n",
    "def prediction(net, loss):\n",
    "    \"\"\"\n",
    "        对数据进行预测\n",
    "    :param net: 训练好的模型\n",
    "    :param loss: 模型的测试误差值, 不是损失函数. 可以去掉, 这里是用来给预测数据命名方便区分.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        pred_loader = torch.utils.data.DataLoader(dataset=pred_data)\n",
    "        res = []\n",
    "        for x in pred_loader:\n",
    "            x = torch.tensor(x, device='cuda:1', dtype=torch.float64)\n",
    "            output = net(x.float())\n",
    "            res.append(output.cpu().numpy().tolist())\n",
    "\n",
    "        res = [i[0] for i in res]\n",
    "        res_df = pd.DataFrame(res, columns=[ 'label_1', 'label_2', 'label_3','label_4','label_5','label_6'])\n",
    "        res_df.insert(0, 'id', value=range(10, 12000))\n",
    "\n",
    "        res_df.to_csv('res-loss '+str(loss)+'.csv', index=False)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            CNN模型构造\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            # input shape(32, 1, 256) -> [batch_size, channel, features]64 1 8192\n",
    "            # 参考->https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),   # 卷积后(32, 16, 256)64 16 8192\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 下采样down-sampling\n",
    "        self.sampling_layer1 = nn.Sequential(\n",
    "            # input shape(32, 16, 256) 8192\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # size随便选的, 这里output应该是(32, 32, 128) 64 32 4096\n",
    "        )\n",
    "\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),   # 输出(32, 64, 128) 64 64 4096\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),  # 输出(32, 128, 128) 64 128 4096\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 64, 64) 2048\n",
    "        )\n",
    "\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),  # 输出(32, 256, 64) 84 256 2048\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),  # 输出(32, 512, 64) 64 512 2048\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 512, 32) 64 512 1024\n",
    "        )\n",
    "        # 全连接层\n",
    "        self.full_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=512*wavetoChn, out_features=256*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256*wavetoChn, out_features=128*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128*wavetoChn, out_features=64*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64*wavetoChn, out_features=6)\n",
    "        )\n",
    "        # 这个是输出label预测概率, 不知道这写法对不对\n",
    "        self.pred_layer = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "        :param x: batch\n",
    "        :return: training == Ture 返回的是全连接层输出， training == False 加上一个Softmax(), 返回各个label概率.\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(dim=1)  # 升维. input shape(32, 205), output shape(32, 1, 205)\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.sampling_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.sampling_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.sampling_layer3(x)\n",
    "        x = x.view(x.size(0), -1)   # output(32, 12800)\n",
    "        x = self.full_layer(x)\n",
    "\n",
    "        if self.training:\n",
    "            return x    # CrossEntropyLoss自带LogSoftmax, 训练的时候不用输出概率(我也不知道这个写法对不对, 我是试错出来的.)\n",
    "        else:\n",
    "            return self.pred_layer(x)\n",
    "\n",
    "\n",
    "class AbsSumLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            可以直接用PyTorch的nn.L1Loss, 这个我写的时候不知道。\n",
    "        \"\"\"\n",
    "        super(AbsSumLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = F.l1_loss(target, output, reduction='sum')\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_random_seed(1996)   # 设定随机种子\n",
    "    # 加载数据集\n",
    "    data = pd.read_csv('../dataset/train1/train1.csv')\n",
    "    data = process_data(data)\n",
    "    pred_data = pd.read_csv('../dataset/train1/test1.csv')\n",
    "    pred_data = get_pred_x(pred_data)\n",
    "\n",
    "    # 初始化模型\n",
    "    lr_rate = 1e-5\n",
    "    w_decay = 1e-6\n",
    "    n_epoch = 100\n",
    "    b_size = 1\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    net = Model()\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=lr_rate, weight_decay=w_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    print(Model)\n",
    "\n",
    "    # 拆分训练测试集\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    train, test = torch.cuda.FloatTensor(train), torch.cuda.FloatTensor(test)\n",
    "    train=train.to(device)\n",
    "    test=test.to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train, batch_size=b_size)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=b_size)\n",
    "    \n",
    "    # vis_graph = h.build_graph(net,[8192])   # 获取绘制图像的对象\n",
    "    # vis_graph.theme = h.graph.THEMES[\"blue\"].copy()     # 指定主题颜色\n",
    "    # vis_graph.save(\"./demo1.png\")   # 保存图像的路径\n",
    "    # print(train_loader.shape)\n",
    "\n",
    "    # summary(net, input_size=(8192))\n",
    "    # net1 = Model()\n",
    "    # net1.to(device)\n",
    "    # print(Model.summary())\n",
    "    # summary(net1, input_size=[(8192, 64)])\n",
    "    # summary(net1, ((8192,64)))\n",
    "    # sampledata = torch.cuda.FloatTensor(32, 8192)\n",
    "    # sampledata=sampledata.to(device)\n",
    "    # # 看看输出结果对不对\n",
    "    # out = net1(sampledata)\n",
    "    # out=\n",
    "    # print(out)\n",
    "    # sys.exit()\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        start = time.time()\n",
    "        print(f\"\\n----------Epoch {epoch + 1}----------\")\n",
    "        train_loop(train_loader, net, loss_fn, optimizer)\n",
    "        test_loop(test_loader, net, loss_fn)\n",
    "        end = time.time()\n",
    "        print('training time: ', end-start)\n",
    "\n",
    "    # predict\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('RockLabTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "527e757e1c587730d72dfd6dc7ea9b8408877a8a8e3b307a272307089f882730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
