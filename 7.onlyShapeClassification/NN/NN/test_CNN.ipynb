{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "#start = time.time()\n",
    "\n",
    "LABELS=[]#标签列表\n",
    "for i in range(16):\n",
    "    LABELS.append(chr(ord('A')+i))\n",
    "                 \n",
    "def show_confusion_matrix(validations, predictions):\n",
    "    '''\n",
    "    生成结果的混淆矩阵\n",
    "    '''\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)  \n",
    "    plt.figure('CNN',figsize=(10, 8))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap=\"coolwarm\",\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                annot_kws={'size':14, 'color':'w'},\n",
    "                fmt=\"d\",)\n",
    "    plt.title(\"Confusion Matrix of CNN_TSC\",fontsize=18)\n",
    "    plt.ylabel(\"True Label\",fontsize=16)\n",
    "    plt.xlabel(\"Predicted Label\",fontsize=16)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    cax = plt.gcf().axes[-1]\n",
    "    cax.tick_params(labelsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "def load_file(filepath):\n",
    "    '''\n",
    "    读取指定文件返回为np数组形式\n",
    "    '''\n",
    "    f =open(file=filepath,mode='rb')\n",
    "    data=[]\n",
    "    for line in f.readlines()[2:]:\n",
    "        num=[]\n",
    "        for s in line.decode('UTF-8').replace('\\n','').split('\\t'):\n",
    "            num.append(np.float(s))\n",
    "        data.append(num)          \n",
    "    f.close() \n",
    "    return np.array(data).T\n",
    "\n",
    "def load_dataset(data_rootdir, dirname,istrain):\n",
    "    '''\n",
    "    遍历路径载入数据集\n",
    "    '''\n",
    "    filename_list = []\n",
    "    filepath_list = []\n",
    "    S=[]\n",
    "   \n",
    "    # 利用os.walk() 方法遍历文件、目录。\n",
    "    for rootdir, dirnames, filenames in os.walk(data_rootdir + dirname):\n",
    "        filenames.sort(key = lambda x: int(x[:-4]))#此步按文件名序号排序         \n",
    "        for filename in filenames:\n",
    "            filename_list.append(filename)\n",
    "            filepath_list.append(os.path.join(rootdir, filename))\n",
    "        #print(filename_list)\n",
    "        #print(filepath_list)\n",
    "                \n",
    "    x=lable_creat(16,int(len(filename_list)/16),4,False)\n",
    "    for i in range(len(filepath_list)):\n",
    "        data=load_file(filepath_list[i])\n",
    "        #加标签\n",
    "        if istrain:\n",
    "            data=np.column_stack((data,np.array(x[i]).T))\n",
    "        S.append(data)\n",
    "        \n",
    "    #乱序\n",
    "    if istrain:\n",
    "        np.random.shuffle(S)\n",
    "       \n",
    "    S=np.vstack((S))        \n",
    "    df=pd.DataFrame(S)\n",
    "    return df\n",
    "\n",
    "def lable_creat(kind_num,train_num,sensor_num,ispre):\n",
    "    '''\n",
    "    为原始数据添加标签\n",
    "    '''\n",
    "    x=[]\n",
    "    z=[]\n",
    "    for i in range(0,kind_num):\n",
    "        for j in range(0,train_num):\n",
    "            z.append(i)\n",
    "            y=[]\n",
    "            for k in range(0,sensor_num):\n",
    "                y.append(i)  \n",
    "            x.append(y)\n",
    "    if ispre:\n",
    "        x=z\n",
    "    return x\n",
    "\n",
    "#数据路径\n",
    "Data_rootdir='C:/Users/wg/Desktop/TSC/data/2/'\n",
    "\n",
    "#把数据文件分批喂入网络训练，确定每次喂多少\n",
    "Batch_size = 5\n",
    "\n",
    "#Long代表总训练数据集数目，Lens代表其中用于训练网络的数据数目（7：3划分）\n",
    "Long = 80   \n",
    "Lens = 56\n",
    "\n",
    "#把标签转成oneHot\n",
    "def convert2oneHot(index,Lens):\n",
    "    hot = np.zeros((Lens,))\n",
    "    hot[int(index)] = 1\n",
    "    return(hot)\n",
    "\n",
    "def xs_gen(path=Data_rootdir,batch_size = Batch_size,train=True,Lens=Lens*4):\n",
    "    '''\n",
    "    训练数据生成器\n",
    "    '''\n",
    "    img_list=load_dataset(path, 'train/',True)\n",
    "    if train:\n",
    "        img_list = np.array(img_list)[:Lens]\n",
    "        print(\"Found %s train items.\"%(int(len(img_list)/4))) #len(img_list)/4 注意此处输入包含4个时间序列\n",
    "        steps = math.ceil((len(img_list)/4) / batch_size)    # 确定每轮有多少个batch\n",
    "    else:\n",
    "        img_list = np.array(img_list)[Lens:]\n",
    "        print(\"Found %s test items.\"%(int(len(img_list)/4)))\n",
    "        steps = math.ceil((len(img_list)/4) / batch_size)    # 确定每轮有多少个batch\n",
    "    while True:\n",
    "        for i in range(steps):\n",
    "\n",
    "            batch_list = img_list[i * batch_size*4 : i * batch_size*4 + batch_size*4] #batch_size*4 此处注意\n",
    "            #np.random.shuffle(batch_list)\n",
    "            x = np.array([file for file in batch_list[:,0:-1]])\n",
    "            y = np.array([convert2oneHot(label,16) for label in batch_list[:,-1]])\n",
    "            batch_x = x.reshape(int(x.shape[0]/4),1050*4) #数据展平\n",
    "            batch_y=[]\n",
    "            for i in range(0,y.shape[0],4):\n",
    "                batch_y.append(y[i,:])\n",
    "            batch_y=np.vstack((batch_y))\n",
    "            yield batch_x, batch_y\n",
    "            \n",
    "def ts_gen(path=Data_rootdir,batch_size = Batch_size):\n",
    "    '''\n",
    "    验证数据生成器\n",
    "    '''\n",
    "    img_list=load_dataset(path, 'test/',False)\n",
    "    img_list = np.array(img_list)\n",
    "    print(\"Found %s test items.\"%(int(len(img_list)/4)))\n",
    "    steps = math.ceil((len(img_list)/4) / batch_size)    # 确定每轮有多少个batch\n",
    "    while True:\n",
    "        for i in range(steps):\n",
    "            batch_list = img_list[i * batch_size*4 : i * batch_size*4 + batch_size*4]\n",
    "            #np.random.shuffle(batch_list)\n",
    "            x = np.array([file for file in batch_list])\n",
    "            #batch_y = np.array([convert2oneHot(label,10) for label in batch_list[:,-1]])\n",
    "            batch_x = x.reshape(int(x.shape[0]/4),1050*4)#数据展平\n",
    "            yield batch_x\n",
    "      \n",
    "TIME_PERIODS = 1050 #数据长度\n",
    "num_sensors=4 #每个输入包含4个时间序列\n",
    "\n",
    "def build_model(num_classes=16):\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((TIME_PERIODS, num_sensors), input_shape=(TIME_PERIODS*num_sensors,))) #输入维度调整\n",
    "    model.add(Conv1D(16, 8,strides=2, activation='relu',input_shape=(TIME_PERIODS,num_sensors))) #输入维度设置\n",
    "    model.add(Conv1D(16, 8,strides=2, activation='relu',padding=\"same\"))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(64, 4,strides=2, activation='relu',padding=\"same\"))\n",
    "    model.add(Conv1D(64, 4,strides=2, activation='relu',padding=\"same\"))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(256, 4,strides=2, activation='relu',padding=\"same\"))\n",
    "    model.add(Conv1D(256, 4,strides=2, activation='relu',padding=\"same\"))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(512, 2,strides=1, activation='relu',padding=\"same\"))\n",
    "    model.add(Conv1D(512, 2,strides=1, activation='relu',padding=\"same\"))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(0.3)) #减小网络对数据微小变化的敏感性，提高泛化性能\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return(model)  \n",
    "\n",
    "Train = True\n",
    "if __name__ == '__main__':\n",
    "    if Train == True:\n",
    "        '''\n",
    "        训练网络\n",
    "        '''\n",
    "        train_iter = xs_gen()\n",
    "        val_iter = xs_gen(train=False)\n",
    "        \n",
    "        #自动保存最佳网络\n",
    "        ckpt = keras.callbacks.ModelCheckpoint(\n",
    "            filepath='best_model.h5',\n",
    "            monitor='val_loss', save_best_only=True,verbose=1)\n",
    "\n",
    "        model = build_model()\n",
    "        opt = Adam(0.0002)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=opt, metrics=['accuracy'])\n",
    "        print(model.summary())\n",
    "\n",
    "        model.fit_generator(\n",
    "            generator=train_iter,\n",
    "            steps_per_epoch=Lens//Batch_size,\n",
    "            epochs=50,\n",
    "            initial_epoch=0,\n",
    "            validation_data = val_iter,\n",
    "            nb_val_samples = (Long - Lens)//Batch_size,\n",
    "            callbacks=[ckpt],\n",
    "            )\n",
    "        model.save(\"finishModel.h5\")\n",
    "    else:\n",
    "        '''\n",
    "        测试网络\n",
    "        '''\n",
    "        test_iter = ts_gen()\n",
    "        model = load_model(\"best_model.h5\")\n",
    "        pres = model.predict_generator(generator=test_iter,steps=math.ceil(48/Batch_size),verbose=1)\n",
    "        print(pres.shape)\n",
    "        ohpres = np.argmax(pres,axis=1)\n",
    "        print(ohpres.shape)\n",
    "        validations=lable_creat(16,3,4,True)\n",
    "        show_confusion_matrix(validations, ohpres)\n",
    "        \n",
    "#end = time.time()\n",
    "#print ('time cost',end-start,'s')      \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('RockLabTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "527e757e1c587730d72dfd6dc7ea9b8408877a8a8e3b307a272307089f882730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
