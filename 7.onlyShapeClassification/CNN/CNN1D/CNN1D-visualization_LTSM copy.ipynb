{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─LSTM: 1-1                              [-1, 64, 2048]            151,044,096\n",
      "├─Linear: 1-2                            [-1, 1024]                2,098,176\n",
      "├─ReLU: 1-3                              [-1, 1024]                --\n",
      "├─BatchNorm1d: 1-4                       [-1, 1024]                2,048\n",
      "├─Linear: 1-5                            [-1, 512]                 524,800\n",
      "├─ReLU: 1-6                              [-1, 512]                 --\n",
      "├─BatchNorm1d: 1-7                       [-1, 512]                 1,024\n",
      "├─Linear: 1-8                            [-1, 6]                   3,078\n",
      "├─BatchNorm1d: 1-9                       [-1, 6]                   12\n",
      "==========================================================================================\n",
      "Total params: 153,673,234\n",
      "Trainable params: 153,673,234\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 153.62\n",
      "==========================================================================================\n",
      "Input size (MB): 2.00\n",
      "Forward/backward pass size (MB): 1.02\n",
      "Params size (MB): 586.22\n",
      "Estimated Total Size (MB): 589.24\n",
      "==========================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tzr/.local/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:4315: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n",
      "/home/tzr/.local/lib/python3.8/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/tzr/.local/lib/python3.8/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/tzr/.local/lib/python3.8/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'onnx_model_name.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import netron\n",
    "\n",
    " \n",
    "wave=8192\n",
    "wavetoChn=int(wave/8)\n",
    "print(wavetoChn)\n",
    "\n",
    "def cal_len(x):\n",
    "    return x//2 -1\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            CNN模型构造\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            # input shape(32, 1, 256) -> [batch_size, channel, features] 8192\n",
    "            # 参考->https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),   # 卷积后(32, 16, 256) 8192\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 下采样down-sampling\n",
    "        self.sampling_layer1 = nn.Sequential(\n",
    "            # input shape(32, 16, 256) 8192\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # size随便选的, 这里output应该是(32, 32, 128) 4096\n",
    "        )\n",
    "\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1),   # 输出(32, 64, 128) 4096\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1),  # 输出(32, 128, 128)\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 64, 64)\n",
    "        )\n",
    "\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1),  # 输出(32, 256, 64)\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1),  # 输出(32, 512, 64)\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 512, 32)\n",
    "        )\n",
    "        # 全连接层\n",
    "        self.full_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=512*wavetoChn, out_features=256*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256*wavetoChn, out_features=128*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128*wavetoChn, out_features=64*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64*wavetoChn, out_features=6)\n",
    "        )\n",
    "        # 这个是输出label预测概率, 不知道这写法对不对\n",
    "        self.pred_layer = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "        :param x: batch\n",
    "        :return: training == Ture 返回的是全连接层输出， training == False 加上一个Softmax(), 返回各个label概率.\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(dim=1)  # 升维. input shape(32, 205), output shape(32, 1, 205)\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.sampling_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.sampling_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.sampling_layer3(x)\n",
    "        x = x.view(x.size(0), -1)   # output(32, 12800)\n",
    "        x = self.full_layer(x)\n",
    "\n",
    "        if self.training:\n",
    "            return x    # CrossEntropyLoss自带LogSoftmax, 训练的时候不用输出概率(我也不知道这个写法对不对, 我是试错出来的.)\n",
    "        else:\n",
    "            return self.pred_layer(x)\n",
    "\n",
    " \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ecgsamples=5000):\n",
    "        super(Net, self).__init__()\n",
    "        self.ecgsamples = ecgsamples\n",
    "        self.conv1 = nn.Conv1d(12, 32, kernel_size=3, stride=1, padding=0)\n",
    "#         self.conv1_bn = nn.batchnorm()\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv1d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(cal_len(cal_len(cal_len(self.ecgsamples)))-3, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)), 2, stride=2, padding=0)\n",
    "        x = F.max_pool1d(F.relu(self.conv2(x)), 2, stride=2, padding=0)\n",
    "        x = F.max_pool1d(F.relu(self.conv2(x)), 2, stride=2, padding=0)\n",
    "        x = F.max_pool1d(F.relu(self.conv2(x)), 2, stride=1, padding=0)\n",
    "        x = x.view(-1, x.shape[-1]*x.shape[-2])\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bi_Lstm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bi_Lstm, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=8192, hidden_size=2048,\n",
    "                            num_layers=3)  # 加了双向，输出的节点数翻2倍\n",
    "        self.l1 = nn.Linear(2048, 1024)  # 特征输入\n",
    "        self.l2 = nn.ReLU()  # 激活函数\n",
    "        self.l3 = nn.BatchNorm1d(1024)  # 批标准化\n",
    "        self.l4 = nn.Linear(1024, 512)\n",
    "        self.l5 = nn.ReLU()\n",
    "        self.l6 = nn.BatchNorm1d(512)\n",
    "        self.l7 = nn.Linear(512, 6)  # 输出6个节点\n",
    "        self.l8 = nn.BatchNorm1d(6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        # 选择最后一个时间点的output\n",
    "        out = self.l1(out[:, -1, :])\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.l5(out)\n",
    "        out = self.l6(out)\n",
    "        out = self.l7(out)\n",
    "        out = self.l8(out)\n",
    "        return out\n",
    "        \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "device = \"cuda:0\"\n",
    "# model = Net(ecgsamples=10000).to(device)\n",
    "# summary(model, input_size=(12, 10000))\n",
    "# model1 =Model()\n",
    "# model1 = model1.to(device)\n",
    "# summary(model1, input_size=(1, 8192))\n",
    "model2 = Bi_Lstm().to(device)\n",
    "# model2 = model2.to(device)\n",
    "data = torch.rand(1,64, 8192)\n",
    "data=data.to(device)\n",
    "\n",
    "summary(model2, data)\n",
    "\n",
    "onnx_path = \"onnx_model_name.onnx\" # 文件名\n",
    "torch.onnx.export(model2, data, onnx_path) # 导出神经网络模型为onnx格式\n",
    "netron.start(onnx_path) # 启动netron"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('RockLabTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "527e757e1c587730d72dfd6dc7ea9b8408877a8a8e3b307a272307089f882730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
