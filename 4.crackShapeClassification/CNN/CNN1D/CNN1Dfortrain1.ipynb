{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "<class '__main__.Model'>\n",
      "\n",
      "----------Epoch 1----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_430377/4186008416.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X, y = x_y[:, :wave].type(torch.float64), torch.tensor(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_MAPPING_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 241>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb#ch0000000vscode-remote?line=273'>274</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb#ch0000000vscode-remote?line=274'>275</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m----------Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m----------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb#ch0000000vscode-remote?line=275'>276</a>\u001b[0m train_loop(train_loader, net, loss_fn, optimizer)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb#ch0000000vscode-remote?line=276'>277</a>\u001b[0m test_loop(test_loader, net, loss_fn)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb#ch0000000vscode-remote?line=277'>278</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32m/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb Cell 1\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb#ch0000000vscode-remote?line=71'>72</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb#ch0000000vscode-remote?line=72'>73</a>\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb#ch0000000vscode-remote?line=73'>74</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226c696e7578536572766572284c414e2928426c756529227d/home/tzr/DataLinux/Documents/GitHubSYNC/RockWaveAnalysis/7.onlyShapeClassification/CNN/CNN1D/CNN1Dfortrain1.ipynb#ch0000000vscode-remote?line=74'>75</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "wave = 512\n",
    "wavetoChn = int(wave/8)\n",
    "print(wavetoChn)\n",
    "\n",
    "# https://blog.csdn.net/WildCatFish/article/details/116228950\n",
    "\n",
    "\n",
    "def set_random_seed(state=1):\n",
    "    \"\"\"\n",
    "        设定随机种子\n",
    "    :param state: 随机种子值\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    gens = (np.random.seed, torch.manual_seed)\n",
    "    for set_state in gens:\n",
    "        set_state(state)\n",
    "\n",
    "\n",
    "def process_data(data):\n",
    "    \"\"\"\n",
    "        处理加载的训练数据DataFrame，去掉id，同时对signals以”，“进行拆分。\n",
    "    :param data: DataFrame, shape(n, 3)\n",
    "    :return: np array, shape(n, 206)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        label = data.iloc[i, 2]\n",
    "        x_res.append(label)\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "\n",
    "def get_pred_x(data):\n",
    "    \"\"\"\n",
    "        处理需要预测数据的DataFrame\n",
    "    :param data: DataFrame, shape(n, 2)\n",
    "    :return: np array, shape(n, 205)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for i in range(data.shape[0]):\n",
    "        x_res = data.iloc[i, 1].split(',')\n",
    "        res.append(x_res)\n",
    "    return np.array(res, dtype=np.float64)\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "        模型训练部分\n",
    "    :param dataloader: 训练数据集\n",
    "    :param model: 训练用到的模型\n",
    "    :param loss_fn: 评估用的损失函数\n",
    "    :param optimizer: 优化器\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    for batch, x_y in enumerate(dataloader):\n",
    "        X, y = x_y[:, :wave].type(torch.float64), torch.tensor(\n",
    "            x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
    "        # 开启梯度\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X.float())\n",
    "            loss = loss_fn(pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "        模型测试部分\n",
    "    :param dataloader: 测试数据集\n",
    "    :param model: 测试模型\n",
    "    :param loss_fn: 损失函数\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct, l1_loss = 0, 0, 0\n",
    "    # 用来计算abs-sum. 等于PyTorch L1Loss-->\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss\n",
    "    l1loss_fn = AbsSumLoss()\n",
    "    with torch.no_grad():   # 关掉梯度\n",
    "        model.eval()\n",
    "        for x_y in dataloader:\n",
    "            X, y = x_y[:, :wave].type(torch.float64), torch.tensor(\n",
    "                x_y[:, wave], dtype=torch.long, device='cuda:1')\n",
    "            # 注意Y和y的区别, Y用来计算L1 loss, y是CrossEntropy loss.\n",
    "            Y = torch.zeros(size=(len(y), 6), device='cuda:1')\n",
    "            for i in range(len(Y)):\n",
    "                Y[i][y[i]] = 1\n",
    "\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()    # 这个是CrossEntropy loss\n",
    "            l1_loss += l1loss_fn(pred, Y).item()    # 这个是abs-sum/L1 loss\n",
    "            # 这个是计算准确率的, 取概率最大值的下标.\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size   # 等于CrossEntropy的reduction='mean', 这里有些多此一举可删掉.\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Results:\\nAccuracy: {(100*correct):>0.1f}% abs-sum loss: {l1_loss:>8f} CroEtr loss: {test_loss:>8f}\")\n",
    "\n",
    "\n",
    "def prediction(net, loss):\n",
    "    \"\"\"\n",
    "        对数据进行预测\n",
    "    :param net: 训练好的模型\n",
    "    :param loss: 模型的测试误差值, 不是损失函数. 可以去掉, 这里是用来给预测数据命名方便区分.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        pred_loader = torch.utils.data.DataLoader(dataset=pred_data)\n",
    "        res = []\n",
    "        for x in pred_loader:\n",
    "            x = torch.tensor(x, device='cuda:1', dtype=torch.float64)\n",
    "            output = net(x.float())\n",
    "            res.append(output.cpu().numpy().tolist())\n",
    "\n",
    "        res = [i[0] for i in res]\n",
    "        res_df = pd.DataFrame(\n",
    "            res, columns=['label_1', 'label_2', 'label_3', 'label_4', 'label_5', 'label_6'])\n",
    "        res_df.insert(0, 'id', value=range(100, 120))\n",
    "\n",
    "        res_df.to_csv('res-loss '+str(loss)+'.csv', index=False)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            CNN模型构造\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            # input shape(32, 1, 256) -> [batch_size, channel, features] 8192\n",
    "            # 参考->https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3,\n",
    "                      padding=1),   # 卷积后(32, 16, 256) 8192\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 下采样down-sampling\n",
    "        self.sampling_layer1 = nn.Sequential(\n",
    "            # input shape(32, 16, 256) 8192\n",
    "            nn.Conv1d(in_channels=16, out_channels=32,\n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            # size随便选的, 这里output应该是(32, 32, 128) 4096\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3,\n",
    "                      padding=1),   # 输出(32, 64, 128) 4096\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=64, out_channels=128,\n",
    "                      kernel_size=3, padding=1),  # 输出(32, 128, 128)\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 64, 64)\n",
    "        )\n",
    "\n",
    "        self.conv_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=128, out_channels=256,\n",
    "                      kernel_size=3, padding=1),  # 输出(32, 256, 64)\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.sampling_layer3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=256, out_channels=512,\n",
    "                      kernel_size=3, padding=1),  # 输出(32, 512, 64)\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # 输出(32, 512, 32)\n",
    "        )\n",
    "        # 全连接层\n",
    "        self.full_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=512*wavetoChn, out_features=256*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256*wavetoChn, out_features=128*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128*wavetoChn, out_features=64*wavetoChn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64*wavetoChn, out_features=6)\n",
    "        )\n",
    "        # 这个是输出label预测概率, 不知道这写法对不对\n",
    "        self.pred_layer = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            前向传播\n",
    "        :param x: batch\n",
    "        :return: training == Ture 返回的是全连接层输出， training == False 加上一个Softmax(), 返回各个label概率.\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(\n",
    "            dim=1)  # 升维. input shape(32, 205), output shape(32, 1, 205)\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.sampling_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.sampling_layer2(x)\n",
    "        x = self.conv_layer3(x)\n",
    "        x = self.sampling_layer3(x)\n",
    "        x = x.view(x.size(0), -1)   # output(32, 12800)\n",
    "        x = self.full_layer(x)\n",
    "\n",
    "        if self.training:\n",
    "            # CrossEntropyLoss自带LogSoftmax, 训练的时候不用输出概率(我也不知道这个写法对不对, 我是试错出来的.)\n",
    "            return x\n",
    "        else:\n",
    "            return self.pred_layer(x)\n",
    "\n",
    "\n",
    "class AbsSumLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            可以直接用PyTorch的nn.L1Loss, 这个我写的时候不知道。\n",
    "        \"\"\"\n",
    "        super(AbsSumLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss = F.l1_loss(target, output, reduction='sum')\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    set_random_seed(1996)   # 设定随机种子\n",
    "    # 加载数据集\n",
    "    traindata = pd.read_csv('/home/tzr/DataLinux-SSD/Dataset/7.onlyShapeClassification/CNN/dataset/train1-CNN/train1.csv')\n",
    "    traindata = process_data(traindata)\n",
    "    testdata = pd.read_csv('/home/tzr/DataLinux-SSD/Dataset/7.onlyShapeClassification/CNN/dataset/train1-CNN/valid1.csv')\n",
    "    testdata = process_data(testdata)\n",
    "    pred_data = pd.read_csv('/home/tzr/DataLinux-SSD/Dataset/7.onlyShapeClassification/CNN/dataset/train1-CNN/test1.csv')\n",
    "    pred_data = get_pred_x(pred_data)\n",
    "\n",
    "    # 初始化模型\n",
    "    lr_rate = 1e-5\n",
    "    w_decay = 1e-6\n",
    "    n_epoch = 100\n",
    "    b_size = 2\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    net = Model()\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=net.parameters(), lr=lr_rate, weight_decay=w_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "    print(Model)\n",
    "\n",
    "    # 拆分训练测试集\n",
    "    train, test = traindata, testdata\n",
    "    train, test = torch.cuda.FloatTensor(train), torch.cuda.FloatTensor(test)\n",
    "    train = train.to(device)\n",
    "    test = test.to(device)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train, batch_size=b_size)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=b_size)\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        start = time.time()\n",
    "        print(f\"\\n----------Epoch {epoch + 1}----------\")\n",
    "        train_loop(train_loader, net, loss_fn, optimizer)\n",
    "        test_loop(test_loader, net, loss_fn)\n",
    "        end = time.time()\n",
    "        print('training time: ', end-start)\n",
    "\n",
    "    # predict\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('RockLabTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "527e757e1c587730d72dfd6dc7ea9b8408877a8a8e3b307a272307089f882730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
